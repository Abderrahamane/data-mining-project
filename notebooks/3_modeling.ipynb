{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modeling\n",
    "\n",
    "## Objective\n",
    "Train multiple classification models to predict video game hits (total_sales >= 1M).\n",
    "\n",
    "## Models\n",
    "1. Logistic Regression (baseline linear classifier)\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. K-Nearest Neighbors (KNN)\n",
    "\n",
    "## Process\n",
    "- Train baseline models with default parameters\n",
    "- Hyperparameter tuning with GridSearchCV\n",
    "- Save trained models and results for evaluation\n"
   ],
   "id": "5f69f347d709e4c2"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    },
    "ExecuteTime": {
     "end_time": "2025-12-23T22:44:45.312977200Z",
     "start_time": "2025-12-23T22:44:45.312977200Z"
    }
   },
   "cell_type": "code",
   "source": "%%sql\n",
   "id": "3629d955b58b7231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:21:21.784139200Z",
     "start_time": "2025-12-24T14:21:21.250644500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ],
   "id": "a687d5992e644029",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Preprocessed Data\n",
   "id": "91a295350c75fc5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:21:24.732597900Z",
     "start_time": "2025-12-24T14:21:24.213178400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load training and test sets\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').values.ravel()\n",
    "\n",
    "# Load scaled versions (for models that need scaling)\n",
    "X_train_scaled = pd.read_csv('../data/processed/X_train_scaled.csv')\n",
    "X_test_scaled = pd.read_csv('../data/processed/X_test_scaled.csv')\n",
    "\n",
    "# Load class weights\n",
    "class_weights_df = pd.read_csv('../data/processed/class_weights.csv')\n",
    "# Convert to dictionary with int keys\n",
    "class_weights = {int(col): class_weights_df[col].values[0] for col in class_weights_df.columns}\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"\\nTarget distribution in training:\")\n",
    "print(pd.Series(y_train).value_counts())\n"
   ],
   "id": "2c09732b40c5ef0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (11790, 53)\n",
      "Test set: (2948, 53)\n",
      "Class weights: {0: np.float64(0.5557126696832579), 1: np.float64(4.98730964467005)}\n",
      "\n",
      "Target distribution in training:\n",
      "0    10608\n",
      "1     1182\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Baseline Models (Without Tuning)\n",
   "id": "63f4b8d78d00d2c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:21:47.065901300Z",
     "start_time": "2025-12-24T14:21:30.568851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "baseline_results = []\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training Baseline Models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # Use scaled data for Logistic Regression and KNN\n",
    "    if name in ['Logistic Regression', 'KNN']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "    # Train on full training set\n",
    "    model.fit(X_tr, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Model': name,\n",
    "        'CV F1 Mean': cv_scores.mean(),\n",
    "        'CV F1 Std': cv_scores.std(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Test Precision: {precision:.4f}\")\n",
    "    print(f\"  Test Recall: {recall:.4f}\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display results table\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(baseline_df.to_string(index=False))\n"
   ],
   "id": "e67955de81b82ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline Models...\n",
      "================================================================================\n",
      "\n",
      "Logistic Regression:\n",
      "  CV F1: 0.3597 (+/- 0.0076)\n",
      "  Test Accuracy: 0.7541\n",
      "  Test Precision: 0.2453\n",
      "  Test Recall: 0.7017\n",
      "  Test F1: 0.3635\n",
      "  Test ROC-AUC: 0.8075\n",
      "\n",
      "Decision Tree:\n",
      "  CV F1: 0.3275 (+/- 0.0226)\n",
      "  Test Accuracy: 0.8453\n",
      "  Test Precision: 0.3115\n",
      "  Test Recall: 0.4508\n",
      "  Test F1: 0.3684\n",
      "  Test ROC-AUC: 0.6762\n",
      "\n",
      "Random Forest:\n",
      "  CV F1: 0.3268 (+/- 0.0293)\n",
      "  Test Accuracy: 0.8779\n",
      "  Test Precision: 0.3774\n",
      "  Test Recall: 0.3390\n",
      "  Test F1: 0.3571\n",
      "  Test ROC-AUC: 0.7956\n",
      "\n",
      "KNN:\n",
      "  CV F1: 0.3215 (+/- 0.0359)\n",
      "  Test Accuracy: 0.9043\n",
      "  Test Precision: 0.5528\n",
      "  Test Recall: 0.2305\n",
      "  Test F1: 0.3254\n",
      "  Test ROC-AUC: 0.7621\n",
      "\n",
      "================================================================================\n",
      "BASELINE RESULTS SUMMARY\n",
      "================================================================================\n",
      "              Model  CV F1 Mean  CV F1 Std  Test Accuracy  Test Precision  Test Recall  Test F1  Test ROC-AUC\n",
      "Logistic Regression    0.359670   0.007605       0.754071        0.245261     0.701695 0.363477      0.807484\n",
      "      Decision Tree    0.327542   0.022632       0.845319        0.311475     0.450847 0.368421      0.676190\n",
      "      Random Forest    0.326792   0.029311       0.877883        0.377358     0.338983 0.357143      0.795560\n",
      "                KNN    0.321531   0.035893       0.904342        0.552846     0.230508 0.325359      0.762068\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Hyperparameter Tuning\n",
   "id": "6aa370713a319039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:25:09.801336100Z",
     "start_time": "2025-12-24T14:21:47.079911200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "tuned_models = {}\n",
    "tuned_results = []\n",
    "\n",
    "print(\"Hyperparameter Tuning with GridSearchCV...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # Use scaled data for Logistic Regression and KNN\n",
    "    if name in ['Logistic Regression', 'KNN']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grids[name],\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_tr, y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    tuned_models[name] = best_model\n",
    "\n",
    "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"  Best CV F1: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    y_pred = best_model.predict(X_te)\n",
    "    y_pred_proba = best_model.predict_proba(X_te)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    tuned_results.append({\n",
    "        'Model': name,\n",
    "        'Best CV F1': grid_search.best_score_,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Test Precision: {precision:.4f}\")\n",
    "    print(f\"  Test Recall: {recall:.4f}\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display tuned results\n",
    "tuned_df = pd.DataFrame(tuned_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNED RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(tuned_df.to_string(index=False))\n"
   ],
   "id": "a50c4ad52827bc99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning with GridSearchCV...\n",
      "================================================================================\n",
      "\n",
      "Logistic Regression:\n",
      "  Best parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "  Best CV F1: 0.3617\n",
      "  Test Accuracy: 0.7578\n",
      "  Test Precision: 0.2473\n",
      "  Test Recall: 0.6949\n",
      "  Test F1: 0.3648\n",
      "  Test ROC-AUC: 0.8065\n",
      "\n",
      "Decision Tree:\n",
      "  Best parameters: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "  Best CV F1: 0.3571\n",
      "  Test Accuracy: 0.7910\n",
      "  Test Precision: 0.2664\n",
      "  Test Recall: 0.6203\n",
      "  Test F1: 0.3727\n",
      "  Test ROC-AUC: 0.7410\n",
      "\n",
      "Random Forest:\n",
      "  Best parameters: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "  Best CV F1: 0.4472\n",
      "  Test Accuracy: 0.8735\n",
      "  Test Precision: 0.4044\n",
      "  Test Recall: 0.5593\n",
      "  Test F1: 0.4694\n",
      "  Test ROC-AUC: 0.8378\n",
      "\n",
      "KNN:\n",
      "  Best parameters: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "  Best CV F1: 0.3468\n",
      "  Test Accuracy: 0.9016\n",
      "  Test Precision: 0.5150\n",
      "  Test Recall: 0.2915\n",
      "  Test F1: 0.3723\n",
      "  Test ROC-AUC: 0.7490\n",
      "\n",
      "================================================================================\n",
      "TUNED RESULTS SUMMARY\n",
      "================================================================================\n",
      "              Model  Best CV F1  Test Accuracy  Test Precision  Test Recall  Test F1  Test ROC-AUC\n",
      "Logistic Regression    0.361733       0.757802        0.247286     0.694915 0.364769      0.806520\n",
      "      Decision Tree    0.357055       0.791045        0.266376     0.620339 0.372709      0.741036\n",
      "      Random Forest    0.447236       0.873474        0.404412     0.559322 0.469417      0.837756\n",
      "                KNN    0.346785       0.901628        0.514970     0.291525 0.372294      0.748991\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Save Trained Models and Results\n",
   "id": "d24dc3e80910e5ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:25:14.667431800Z",
     "start_time": "2025-12-24T14:25:13.921150400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save baseline models (trained models from section 2)\n",
    "for name, model in models.items():\n",
    "    safe_name = name.replace(' ', '_').lower()\n",
    "    model_path = f'../models/{safe_name}_baseline.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved: {model_path}\")\n",
    "\n",
    "# Save tuned models\n",
    "for name, model in tuned_models.items():\n",
    "    safe_name = name.replace(' ', '_').lower()\n",
    "    model_path = f'../models/{safe_name}_tuned.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved: {model_path}\")\n",
    "\n",
    "# Save results dataframes\n",
    "baseline_df.to_csv('../models/baseline_results.csv', index=False)\n",
    "tuned_df.to_csv('../models/tuned_results.csv', index=False)\n",
    "print(\"\\nResults saved:\")\n",
    "print(\"  - ../models/baseline_results.csv\")\n",
    "print(\"  - ../models/tuned_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total models trained: {len(models)} baseline + {len(tuned_models)} tuned = {len(models)*2}\")\n",
    "print(\"All models saved to: ../models/\")\n",
    "print(\"\\nNext step: Run 4_evaluation.ipynb for detailed model evaluation and comparison\")\n",
    "\n"
   ],
   "id": "f11da22d7ee9cee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../models/logistic_regression_baseline.pkl\n",
      "Saved: ../models/decision_tree_baseline.pkl\n",
      "Saved: ../models/random_forest_baseline.pkl\n",
      "Saved: ../models/knn_baseline.pkl\n",
      "Saved: ../models/logistic_regression_tuned.pkl\n",
      "Saved: ../models/decision_tree_tuned.pkl\n",
      "Saved: ../models/random_forest_tuned.pkl\n",
      "Saved: ../models/knn_tuned.pkl\n",
      "\n",
      "Results saved:\n",
      "  - ../models/baseline_results.csv\n",
      "  - ../models/tuned_results.csv\n",
      "\n",
      "================================================================================\n",
      "MODEL TRAINING COMPLETE\n",
      "================================================================================\n",
      "Total models trained: 4 baseline + 4 tuned = 8\n",
      "All models saved to: ../models/\n",
      "\n",
      "Next step: Run 4_evaluation.ipynb for detailed model evaluation and comparison\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
