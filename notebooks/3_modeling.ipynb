{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modeling\n",
    "\n",
    "## Objective\n",
    "Train and evaluate multiple classification models to predict video game hits (total_sales >= 1M).\n",
    "\n",
    "## Models\n",
    "1. Logistic Regression (baseline linear classifier)\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. K-Nearest Neighbors (KNN)\n",
    "\n",
    "## Evaluation\n",
    "- Cross-validation\n",
    "- Hyperparameter tuning with GridSearchCV\n",
    "- Multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n"
   ],
   "id": "5f69f347d709e4c2"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%%sql\n",
   "id": "3629d955b58b7231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ],
   "id": "a687d5992e644029"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Preprocessed Data\n",
   "id": "91a295350c75fc5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load training and test sets\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').values.ravel()\n",
    "\n",
    "# Load scaled versions (for models that need scaling)\n",
    "X_train_scaled = pd.read_csv('../data/processed/X_train_scaled.csv')\n",
    "X_test_scaled = pd.read_csv('../data/processed/X_test_scaled.csv')\n",
    "\n",
    "# Load class weights\n",
    "class_weights = pd.read_csv('../data/processed/class_weights.csv').to_dict('records')[0]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"\\nTarget distribution in training:\")\n",
    "print(pd.Series(y_train).value_counts())\n"
   ],
   "id": "2c09732b40c5ef0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Baseline Models (Without Tuning)\n",
   "id": "1854ca038f29d98e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=class_weights[1]/class_weights[0],\n",
    "                             eval_metric='logloss', random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "baseline_results = []\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training Baseline Models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # Use scaled data for Logistic Regression and KNN\n",
    "    if name in ['Logistic Regression', 'KNN']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "    # Train on full training set\n",
    "    model.fit(X_tr, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Model': name,\n",
    "        'CV F1 Mean': cv_scores.mean(),\n",
    "        'CV F1 Std': cv_scores.std(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Test Precision: {precision:.4f}\")\n",
    "    print(f\"  Test Recall: {recall:.4f}\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display results table\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(baseline_df.to_string(index=False))\n"
   ],
   "id": "4bb9790a7986f8ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Visualize Baseline Results\n",
   "id": "2af97e61b21b20c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    baseline_df.plot(x='Model', y=metric, kind='barh', ax=ax, legend=False, color='skyblue')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xlim([0, 1])\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(baseline_df[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "baseline_df.plot(x='Model', y='Test ROC-AUC', kind='barh', legend=False, color='coral')\n",
    "plt.xlabel('ROC-AUC Score')\n",
    "plt.title('ROC-AUC Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(baseline_df['Test ROC-AUC']):\n",
    "    plt.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "443ebea458840a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Hyperparameter Tuning\n",
   "id": "89e61860c80a890d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "tuned_models = {}\n",
    "tuned_results = []\n",
    "\n",
    "print(\"Hyperparameter Tuning with GridSearchCV...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # Use scaled data for Logistic Regression and KNN\n",
    "    if name in ['Logistic Regression', 'KNN']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grids[name],\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_tr, y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    tuned_models[name] = best_model\n",
    "\n",
    "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"  Best CV F1: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    y_pred = best_model.predict(X_te)\n",
    "    y_pred_proba = best_model.predict_proba(X_te)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    tuned_results.append({\n",
    "        'Model': name,\n",
    "        'Best CV F1': grid_search.best_score_,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Test Precision: {precision:.4f}\")\n",
    "    print(f\"  Test Recall: {recall:.4f}\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display tuned results\n",
    "tuned_df = pd.DataFrame(tuned_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNED RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(tuned_df.to_string(index=False))\n"
   ],
   "id": "a50c4ad52827bc99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Compare Baseline vs Tuned\n",
   "id": "e19280366748dac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Comparison\n",
    "comparison = baseline_df[['Model', 'Test F1']].merge(\n",
    "    tuned_df[['Model', 'Test F1']],\n",
    "    on='Model',\n",
    "    suffixes=('_Baseline', '_Tuned')\n",
    ")\n",
    "comparison['Improvement'] = comparison['Test F1_Tuned'] - comparison['Test F1_Baseline']\n",
    "\n",
    "print(\"Baseline vs Tuned Model Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison['Test F1_Baseline'], width, label='Baseline', color='lightblue')\n",
    "ax.bar(x + width/2, comparison['Test F1_Tuned'], width, label='Tuned', color='orange')\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Baseline vs Tuned Models Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "32d136f1d7fc3cba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Best Model Analysis\n",
   "id": "94bc60e6be664e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select best model based on F1 score\n",
    "best_model_name = tuned_df.loc[tuned_df['Test F1'].idxmax(), 'Model']\n",
    "best_model = tuned_models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best F1 Score: {tuned_df['Test F1'].max():.4f}\")\n",
    "\n",
    "# Use appropriate data\n",
    "if best_model_name in ['Logistic Regression', 'KNN']:\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    X_test_final = X_test\n",
    "\n",
    "# Predictions\n",
    "y_pred_best = best_model.predict(X_test_final)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_final)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred_best\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks([0.5, 1.5], ['Miss (0)', 'Hit (1)'])\n",
    "plt.yticks([0.5, 1.5], ['Miss (0)', 'Hit (1)'])\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Miss', 'Hit']))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve - {best_model_name}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ],
   "id": "7b9d07475655caa3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Feature Importance (for tree-based models)\n",
   "id": "fdb7430edbc8d00f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if best_model_name in ['Decision Tree', 'Random Forest', 'XGBoost']:\n",
    "    # Get feature importances\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_names = X_train.columns\n",
    "\n",
    "        # Create dataframe\n",
    "        feature_imp_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(feature_imp_df['Feature'], feature_imp_df['Importance'], color='teal')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Top 20 Most Important Features:\")\n",
    "        print(feature_imp_df.to_string(index=False))\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")\n"
   ],
   "id": "ec77ddd294343772"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Save Best Model\n",
   "id": "6f560ee6509075b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "\n",
    "# Save best model\n",
    "model_path = '../models/best_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model ({best_model_name}) saved to: {model_path}\")\n",
    "\n",
    "# Save all tuned models\n",
    "for name, model in tuned_models.items():\n",
    "    safe_name = name.replace(' ', '_').lower()\n",
    "    model_path = f'../models/{safe_name}_tuned.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"{name} saved to: {model_path}\")\n",
    "\n",
    "# Save results\n",
    "baseline_df.to_csv('../models/baseline_results.csv', index=False)\n",
    "tuned_df.to_csv('../models/tuned_results.csv', index=False)\n",
    "print(\"\\nResults saved to models directory\")\n"
   ],
   "id": "f11da22d7ee9cee7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Final Summary\n",
   "id": "c62d3043199e7838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODELING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š Models Trained:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(f\"\\nğŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   - Test F1 Score: {tuned_df['Test F1'].max():.4f}\")\n",
    "print(f\"   - Test ROC-AUC: {tuned_df.loc[tuned_df['Model'] == best_model_name, 'Test ROC-AUC'].values[0]:.4f}\")\n",
    "print(f\"   - Test Accuracy: {tuned_df.loc[tuned_df['Model'] == best_model_name, 'Test Accuracy'].values[0]:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Performance Improvements (Baseline â†’ Tuned):\")\n",
    "for idx, row in comparison.iterrows():\n",
    "    improvement = row['Improvement']\n",
    "    sign = \"ğŸ“ˆ\" if improvement > 0 else \"ğŸ“‰\" if improvement < 0 else \"â¡ï¸\"\n",
    "    print(f\"   {sign} {row['Model']}: {improvement:+.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saved Files:\")\n",
    "print(\"   - Best model: models/best_model.pkl\")\n",
    "print(\"   - All tuned models: models/*_tuned.pkl\")\n",
    "print(\"   - Results: models/baseline_results.csv, models/tuned_results.csv\")\n",
    "\n",
    "print(\"\\nâœ… Modeling complete! Ready for evaluation.\")\n",
    "print(\"=\"*80)\n"
   ],
   "id": "5147128034942d78"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
