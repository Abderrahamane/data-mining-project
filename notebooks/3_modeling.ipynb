{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modeling\n",
    "\n",
    "## Objective\n",
    "Train and evaluate multiple classification models to predict video game hits (total_sales >= 1M).\n",
    "\n",
    "## Models\n",
    "1. Logistic Regression (baseline linear classifier)\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "5. K-Nearest Neighbors (KNN)\n",
    "\n",
    "## Evaluation\n",
    "- Cross-validation\n",
    "- Hyperparameter tuning with GridSearchCV\n",
    "- Multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n"
   ],
   "id": "5f69f347d709e4c2"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    },
    "ExecuteTime": {
     "end_time": "2025-12-23T22:44:45.312977200Z",
     "start_time": "2025-12-23T22:44:45.312977200Z"
    }
   },
   "cell_type": "code",
   "source": "%%sql\n",
   "id": "3629d955b58b7231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T22:44:47.416110500Z",
     "start_time": "2025-12-23T22:44:47.399107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ],
   "id": "a687d5992e644029",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Preprocessed Data\n",
   "id": "91a295350c75fc5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T22:45:01.968490Z",
     "start_time": "2025-12-23T22:45:00.133745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load training and test sets\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').values.ravel()\n",
    "\n",
    "# Load scaled versions (for models that need scaling)\n",
    "X_train_scaled = pd.read_csv('../data/processed/X_train_scaled.csv')\n",
    "X_test_scaled = pd.read_csv('../data/processed/X_test_scaled.csv')\n",
    "\n",
    "# Load class weights\n",
    "class_weights = pd.read_csv('../data/processed/class_weights.csv').to_dict('records')[0]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"\\nTarget distribution in training:\")\n",
    "print(pd.Series(y_train).value_counts())\n"
   ],
   "id": "2c09732b40c5ef0e",
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mEmptyDataError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load training and test sets\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m X_train = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m../data/processed/X_train.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m X_test = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m../data/processed/X_test.csv\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      4\u001B[39m y_train = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m../data/processed/y_train.csv\u001B[39m\u001B[33m'\u001B[39m).values.ravel()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\data-mining-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\data-mining-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\data-mining-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\data-mining-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1895\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1897\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1898\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1899\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1900\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\data-mining-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001B[39m, in \u001B[36mCParserWrapper.__init__\u001B[39m\u001B[34m(self, src, **kwds)\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwds[\u001B[33m\"\u001B[39m\u001B[33mdtype_backend\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     91\u001B[39m     \u001B[38;5;66;03m# Fail here loudly instead of in cython after reading\u001B[39;00m\n\u001B[32m     92\u001B[39m     import_optional_dependency(\u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28mself\u001B[39m._reader = \u001B[43mparsers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.unnamed_cols = \u001B[38;5;28mself\u001B[39m._reader.unnamed_cols\n\u001B[32m     97\u001B[39m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:581\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader.__cinit__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mEmptyDataError\u001B[39m: No columns to parse from file"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Baseline Models (Without Tuning)\n",
   "id": "1854ca038f29d98e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=class_weights[1]/class_weights[0],\n",
    "                             eval_metric='logloss', random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "baseline_results = []\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training Baseline Models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # Use scaled data for Logistic Regression and KNN\n",
    "    if name in ['Logistic Regression', 'KNN']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "    # Train on full training set\n",
    "    model.fit(X_tr, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Model': name,\n",
    "        'CV F1 Mean': cv_scores.mean(),\n",
    "        'CV F1 Std': cv_scores.std(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Test Precision: {precision:.4f}\")\n",
    "    print(f\"  Test Recall: {recall:.4f}\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display results table\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(baseline_df.to_string(index=False))\n"
   ],
   "id": "4bb9790a7986f8ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Visualize Baseline Results\n",
   "id": "2af97e61b21b20c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    baseline_df.plot(x='Model', y=metric, kind='barh', ax=ax, legend=False, color='skyblue')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xlim([0, 1])\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(baseline_df[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "baseline_df.plot(x='Model', y='Test ROC-AUC', kind='barh', legend=False, color='coral')\n",
    "plt.xlabel('ROC-AUC Score')\n",
    "plt.title('ROC-AUC Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(baseline_df['Test ROC-AUC']):\n",
    "    plt.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "443ebea458840a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Hyperparameter Tuning\n",
   "id": "89e61860c80a890d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "tuned_models = {}\n",
    "tuned_results = []\n",
    "\n",
    "print(\"Hyperparameter Tuning with GridSearchCV...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    # Use scaled data for Logistic Regression and KNN\n",
    "    if name in ['Logistic Regression', 'KNN']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grids[name],\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_tr, y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    tuned_models[name] = best_model\n",
    "\n",
    "    print(f\"  Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"  Best CV F1: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    y_pred = best_model.predict(X_te)\n",
    "    y_pred_proba = best_model.predict_proba(X_te)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    tuned_results.append({\n",
    "        'Model': name,\n",
    "        'Best CV F1': grid_search.best_score_,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Precision': precision,\n",
    "        'Test Recall': recall,\n",
    "        'Test F1': f1,\n",
    "        'Test ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Test Precision: {precision:.4f}\")\n",
    "    print(f\"  Test Recall: {recall:.4f}\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Display tuned results\n",
    "tuned_df = pd.DataFrame(tuned_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNED RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(tuned_df.to_string(index=False))\n"
   ],
   "id": "a50c4ad52827bc99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Compare Baseline vs Tuned\n",
   "id": "e19280366748dac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Comparison\n",
    "comparison = baseline_df[['Model', 'Test F1']].merge(\n",
    "    tuned_df[['Model', 'Test F1']],\n",
    "    on='Model',\n",
    "    suffixes=('_Baseline', '_Tuned')\n",
    ")\n",
    "comparison['Improvement'] = comparison['Test F1_Tuned'] - comparison['Test F1_Baseline']\n",
    "\n",
    "print(\"Baseline vs Tuned Model Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison['Test F1_Baseline'], width, label='Baseline', color='lightblue')\n",
    "ax.bar(x + width/2, comparison['Test F1_Tuned'], width, label='Tuned', color='orange')\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Baseline vs Tuned Models Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "32d136f1d7fc3cba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Best Model Analysis\n",
   "id": "94bc60e6be664e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select best model based on F1 score\n",
    "best_model_name = tuned_df.loc[tuned_df['Test F1'].idxmax(), 'Model']\n",
    "best_model = tuned_models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best F1 Score: {tuned_df['Test F1'].max():.4f}\")\n",
    "\n",
    "# Use appropriate data\n",
    "if best_model_name in ['Logistic Regression', 'KNN']:\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    X_test_final = X_test\n",
    "\n",
    "# Predictions\n",
    "y_pred_best = best_model.predict(X_test_final)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_final)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred_best\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks([0.5, 1.5], ['Miss (0)', 'Hit (1)'])\n",
    "plt.yticks([0.5, 1.5], ['Miss (0)', 'Hit (1)'])\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Miss', 'Hit']))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve - {best_model_name}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ],
   "id": "7b9d07475655caa3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Feature Importance (for tree-based models)\n",
   "id": "fdb7430edbc8d00f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if best_model_name in ['Decision Tree', 'Random Forest', 'XGBoost']:\n",
    "    # Get feature importances\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_names = X_train.columns\n",
    "\n",
    "        # Create dataframe\n",
    "        feature_imp_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(feature_imp_df['Feature'], feature_imp_df['Importance'], color='teal')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Top 20 Most Important Features:\")\n",
    "        print(feature_imp_df.to_string(index=False))\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")\n"
   ],
   "id": "ec77ddd294343772"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Save Best Model\n",
   "id": "6f560ee6509075b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "\n",
    "# Save best model\n",
    "model_path = '../models/best_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model ({best_model_name}) saved to: {model_path}\")\n",
    "\n",
    "# Save all tuned models\n",
    "for name, model in tuned_models.items():\n",
    "    safe_name = name.replace(' ', '_').lower()\n",
    "    model_path = f'../models/{safe_name}_tuned.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"{name} saved to: {model_path}\")\n",
    "\n",
    "# Save results\n",
    "baseline_df.to_csv('../models/baseline_results.csv', index=False)\n",
    "tuned_df.to_csv('../models/tuned_results.csv', index=False)\n",
    "print(\"\\nResults saved to models directory\")\n"
   ],
   "id": "f11da22d7ee9cee7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Final Summary\n",
   "id": "c62d3043199e7838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODELING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š Models Trained:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(f\"\\nğŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   - Test F1 Score: {tuned_df['Test F1'].max():.4f}\")\n",
    "print(f\"   - Test ROC-AUC: {tuned_df.loc[tuned_df['Model'] == best_model_name, 'Test ROC-AUC'].values[0]:.4f}\")\n",
    "print(f\"   - Test Accuracy: {tuned_df.loc[tuned_df['Model'] == best_model_name, 'Test Accuracy'].values[0]:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Performance Improvements (Baseline â†’ Tuned):\")\n",
    "for idx, row in comparison.iterrows():\n",
    "    improvement = row['Improvement']\n",
    "    sign = \"ğŸ“ˆ\" if improvement > 0 else \"ğŸ“‰\" if improvement < 0 else \"â¡ï¸\"\n",
    "    print(f\"   {sign} {row['Model']}: {improvement:+.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Saved Files:\")\n",
    "print(\"   - Best model: models/best_model.pkl\")\n",
    "print(\"   - All tuned models: models/*_tuned.pkl\")\n",
    "print(\"   - Results: models/baseline_results.csv, models/tuned_results.csv\")\n",
    "\n",
    "print(\"\\nâœ… Modeling complete! Ready for evaluation.\")\n",
    "print(\"=\"*80)\n"
   ],
   "id": "5147128034942d78"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
